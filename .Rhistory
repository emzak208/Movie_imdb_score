wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
## Example: Classifying Cancer Samples ----
## Step 2: Exploring and preparing the data ----
# import the CSV file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# examine the structure of the wbcd data frame
str(wbcd)
# drop the id feature
wbcd <- wbcd[-1]
# table of diagnosis
table(wbcd$diagnosis)
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# summarize three numeric features
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# create normalization function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# test normalization function - result should be identical
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# confirm that normalization worked
summary(wbcd_n$area_mean)
# create training and test data
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# create labels for training and test data
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
# visualize the data using labels
plot(wbcd$radius_mean,wbcd$texture_mean,
main = 'Scatterplot',
xlab = 'randius mean',
ylab = 'texture mean')
pairs(~radius_mean+texture_mean+perimeter_mean+area_mean+smoothness_mean,
data = wbcd,
main = 'Scaterplot of many variables')
library(car)
scatterplot(texture_mean ~ radius_mean | diagnosis, data = wbcd,
main = 'Scatterplot',
xlab = 'randius mean',
ylab = 'texture mean')
scatterplotMatrix(~radius_mean+texture_mean+perimeter_mean+area_mean+smoothness_mean | diagnosis, data=wbcd)
## Step 3: Training a model on the data ----
# load the "class" library  #how to read the result from this step
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
head(wbcd_test)
head(wbcd_test_pred)   # predictions: predicting the class of examples of testing data#
#  accuracy (chapter 10 p318) for k=21 values
## Step 4: Evaluating model performance ----
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual    ## this is the anser to question 3 in hw 2
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq = FALSE)
## Step 5: Improving model performance ----
# use the scale() function to z-score standardize a data frame
wbcd_z <- as.data.frame(scale(wbcd[-1]))
# confirm that the transformation was applied correctly
summary(wbcd_z$area_mean)
# create training and test datasets
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
# re-classify test cases
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq = FALSE)
# try several different values of k
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
library(alr4)
data('UN11')
## a)
#ppgdp is the predictor and fertility is the response.
## b)
ggplot(UN11,aes(x=ppgdp,y=fertility))+geom_point(color='blue',size=1)
#or
plot(fertility~ppgdp,UN11)
abline(lm(fertility~ppgdp,UN11))
# Summary: we cannot tell there's linear relationship between x and y
## c)
log_f<-log(UN11$fertility)
log_g<-log(UN11$ppgdp)
ggplot(UN11,aes(x=log_g,y=log_f))+geom_point(color='blue',size=1)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library(alr4)
data('UN11')
## a)
#ppgdp is the predictor and fertility is the response.
## b)
ggplot(UN11,aes(x=ppgdp,y=fertility))+geom_point(color='blue',size=1)
#or
plot(fertility~ppgdp,UN11)
abline(lm(fertility~ppgdp,UN11))
# Summary: we cannot tell there's linear relationship between x and y
## c)
log_f<-log(UN11$fertility)
log_g<-log(UN11$ppgdp)
ggplot(UN11,aes(x=log_g,y=log_f))+geom_point(color='blue',size=1)
plot(log_f~log_g,UN11)
View(UN11)
library(alr4)
data('UN11')
library(alr4)
data('UN11')
('UN11')
library(alr4)
data('UN11')
UN11
View(UN11)
View(UN11)
log_g
library(e1071)
library(e1071)
data(HouseVotes84)
data(HouseVotes84)
data(HouseVotes84
)
library(e1071)
data(HouseVotes84)
install.packages("mlbench")
library(mlbench)
data("HouseVotes84")
library(mlbench)
data('HouseVotes84')
View(HouseVotes84)
View(HouseVotes84)
data('HouseVotes84',header=TRUE)
library(mlbench)
data('HouseVotes84')
View(HouseVotes84)
View(HouseVotes84)
data(HouseVotes84)
View(HouseVotes84)
View(HouseVotes84)
summary(HouseVotes84)
library(e1071)
data(HouseVotes84, package = "mlbench")
library(e1071)
data(HouseVotes84, package = "mlbench")
View(HouseVotes84)
#Set training data set and test data set
# I set the first 75% of 435 observatiosn as training, the rest is test
hv_train<-HouseVotes84[1:326,-1]
hv_test<-HouseVotes84[327:435,-1]
m <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
m <- naiveBayes(hv_train, HouseVotes84$Class)
hv_train_labels <- sms_raw[1:326,-1 ]$class # label data by their types: ham or spam
hv_test_labels<- sms_raw[327:435,-1 ]$class
hv_train_labels <- HouseVotes84[1:326,-1 ]$class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435,-1 ]$class
View(HouseVotes84)
sms_train_labels
hv_train_labels
View(HouseVotes84)
hv_train_labels <- HouseVotes84[1:326,-1 ]$Class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435,-1 ]$Class
HouseVotes84$Class <- factor(HouseVotes84$Class)
hv_train_labels <- HouseVotes84[1:326,-1 ]$Class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435,-1 ]$Class
hv_train_labels
library(mlbench)
data("HouseVotes84")
summary(HouseVotes84)
# save label
HouseVotes84$Class <- factor(HouseVotes84$Class)
hv_train_labels <- HouseVotes84[1:326,-1 ]$Class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435,-1 ]$Class
hv_train_labels
# set train and test
hv_train<-HouseVotes84[1:326,-1]
hv_test<-HouseVotes84[327:435,-1]
model <- naiveBayes(hv_train, HouseVotes84$Class)
predict(model, HouseVotes84[1:10,-1])
predict(model, HouseVotes84[1:10,-1], type = "raw")
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
model <- naiveBayes(hv_train, HouseVotes84$Class)
model <- naiveBayes(hv_train, HouseVotes84$Class)
model<- naiveBayes(hv_train, HouseVotes84$Class)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
predict(model, HouseVotes84[1:10,-1], type = "raw")
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
View(HouseVotes84)
library(e1071)
HouseVotes84$Class <- factor(HouseVotes84$Class)
hv_train_labels <- HouseVotes84[1:326,-1 ]$Class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435,-1 ]$Class
hv_train_labels <- HouseVotes84[1:326, ]$Class # label data by their types: ham or spam
hv_test_labels<- HouseVotes84[327:435, ]$Class
hv_classifier <- naiveBayes(hv_train, hv_train_labels)
# Evaluate model performance
hv_test_pred <- predict(hv_classifier, hv_test)
head(sms_test_pred)
hv_classifier <- naiveBayes(hv_train, hv_train_labels)
# Evaluate model performance
hv_test_pred <- predict(hv_classifier, hv_test)
head(hv_test_pred)
View(HouseVotes84)
library(gmodels)
CrossTable(hv_test_pred, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Step 5-Improving model performance
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 1)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Step 5-Improving model performance
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 3)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Step 5-Improving model performance
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 5)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Step 5-Improving model performance
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 10)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Step 5-Improving model performance
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 3)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
hv_classifier <- naiveBayes(hv_train, hv_train_labels)
hv_test_pred <- predict(hv_classifier, hv_test)
head(hv_test_pred)
library(gmodels)
CrossTable(hv_test_pred, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
hv_classifier <- naiveBayes(hv_train, hv_train_labels)
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 3)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
hv_test_pred <- predict(hv_classifier, hv_test)
head(hv_test_pred)
library(gmodels)
CrossTable(hv_test_pred, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
hv_test_pred
hv_classifier2 <- naiveBayes(hv_train, hv_train_labels, laplace = 3)
hv_test_pred2 <- predict(hv_classifier2, hv_test)
CrossTable(hv_test_pred2, hv_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
hv_test_pred2
install.packages("C50")
library(C50)
sudo R CMD javareconf
sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
sudo R CMD javareconf
install.packages("RWeka")
install.packages("rJava",type='source
))
install.packages("rJava",type='source')
install.packages("rJava",type='source')
install.packages("RWeka")
install.packages("RWeka")
install.packages("RWeka")
install.packages("RWeka")
install.packages("sqldb")
install.packages("sqldf")
install.packages("dplyr")
install.packages("tidyr")
library(astsa)
rec.mle = ar.mle(rec, order=2) # using maximum likelyhhod method to estimat fi1 and fi2
rec.mle
rec.mle
rec.mle$x.mean[1]
rec.mle$ar[1]
sqrt(diag(rec.mle$asy.var.coef))
rec.mle$var.pred[1]
# fit in an Arima(p,d,q) --->  arima(data,order=(p,d,q))
rec.arima = arima(rec, order=c(2,0,0))
rec.arima
library(prodn)
plot(prodn)
## Example : Australian Beer
library(fpp)
install.packages("fpp")
library(fpp)
library(fpp)
plot(ausbeer)
seasonplot(ausbeer)
lbeer = log(ausbeer)# no difference after log transformation
plot(lbeer)
beer.fit = sarima(lbeer, 1,1,1,1,1,0,4)
names(beer.fit)
library(astsa)
rec.mle = ar.mle(rec, order=2) # using maximum likelyhood method to estimat fi1 and fi2
rec.mle
rec.mle$x.mean
rec.mle$ar
sqrt(diag(rec.mle$asy.var.coef))
rec.mle$var.pred
rec.arima = arima(rec, order=c(2,0,0))
rec.arima
rec.yw = ar.yw(rec, order=2)
rec.arima = arima(rec, order=c(2,0,0))
rec.arima
rec.yw = ar.yw(rec, order=2)
rec.yw
acf(rec.yw$resid, na.action=na.pass)
pacf(rec.yw$resid, na.action=na.pass)
plot(prodn)
plot(diff(prodn))
acf(diff(prodn))
pacf(diff(prodn))
fed = log(prodn) # take log get rid of trend then diff
plot(diff(fed))
acf(diff(fed))
pacf(diff(fed))
par(mfrow=c(2,1))
plot(diff(prodn))
plot(diff(fed))
par(mfrow=c(1,1))
fed.fit = arima(fed, order =c(1,1,1), seasonal=list(order=c(2,1,1), period=12))
fed.fit
tsdiag(fed.fit, fog.lag=48)
par(mfrow=c(1,1))
tsdiag(fed.fit, fog.lag=48)
acf(fed.fit$res)
pacf(fed.fit$res)
fed.pr = predict(fed.fit, n.ahead=24) # get prediction for 24 months later
fed.pr$pred
?fed.pr
str(fed.pr)
summary(fed.pr)
fed.fit2 = sarima(fed,1,1,1,2,1,1,12)
fed.fit2$fit
fed.pr2 = sarima.for(fed,24,1,1,1,2,1,1,12) # Predict for 2 months
fed.pr2$pred
library(fpp)
plot(ausbeer)
seasonplot(ausbeer)
lbeer = log(ausbeer) # no difference after log transformation
plot(lbeer)
beer.fit = sarima(lbeer, 1,1,1,1,1,0,4)
names(beer.fit)
sarima.for(lbeer, 8,1,1,1,1,1,0,4)$pred
beer.fit = sarima(ausbeer, 1,1,1,1,1,0,4)
sarima.for(ausbeer, 8, 1,1,1,1,1,0,4)$pred
names(sarima.for())
names(sarima.for$pred)
names(sarima.for(lbeer, 8,1,1,1,1,1,0,4)$pred)
names(sarima.for(lbeer, 8,1,1,1,1,1,0,4))
fed.pr2$pred
pr3=sarima.for(lbeer, 8,1,1,1,1,1,0,4)
prd$pred
pr3=sarima.for(lbeer, 8,1,1,1,1,1,0,4)
pr3$pred
interval(pr3$pred)
fed.pr = predict(fed.fit, n.ahead=24) # get prediction for 24 months later
fed.pr$pred
predict(fed.fit, n.ahead=24)$pred
sarima.for(fed,24,1,1,1,2,1,1,12)$pred
sarima.for(fed,24,1,1,1,2,1,1,12)$pred
sarima.for(fed,24,1,1,1,2,1,1,12)$pred
launch <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml10/challenger.csv")
# exammine the launch data
str(launch)
View(launch)
launch$distress_ct = ifelse(launch$distress_ct<1,0,1)
launch$distress_ct
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx
launch <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml10/challenger.csv")
# examine the launch data
str(launch)
?sample()
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx # ramdomize rows, save 90% of data into index
launch_train = launch[indx,]
launch_test = launch[-indx,]
launch_train_labels = launch[indx,1]
launch_test_labels = launch[-indx,1]
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx # ramdomize rows, save 90% of data into index
launch_train = launch[indx,]
launch_test = launch[-indx,]
launch_train_labels = launch[indx,1]
launch_test_labels = launch[-indx,1]
View(launch)
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx # ramdomize rows, save 90% of data into index
launch_train = launch[indx,]
launch_test = launch[-indx,]
launch_train_labels = launch[indx,1] # label the first column: distress_ct is the chategorial dependent variable
launch_test_labels = launch[-indx,1]
library(Amelia)
missmap(launch, main = "Missing values vs observed")
sapply(launch,function(x) sum(is.na(x)))
sapply(launch, function(x) length(unique(x)))
model <- glm(distress_ct~.,family=binomial(link='logit'),data=launch_trai)
model <- glm(distress_ct~.,family=binomial(link='logit'),data=launch_train)
model <- glm(distress_ct~.,family=binomial(link='logit'),
data=launch_trai)
model <- glm(distress_ct~.,family=binomial(link='logit'),
data=launch_train)
model <- glm(distress_ct~,family=binomial(link='logit'),
model <- glm(distress_ct~family=binomial(link='logit'),
View(launch)
model <- glm(distress_ct~.,family=binomial(link='logit'),
data=launch_train)
model<- glm(distress_ct~.,family=binomial(link='logit'),data=launch_train)
## Example: Space Shuttle Launch Data ----
launch <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml10/challenger.csv")
# examine the launch data
str(launch)
# logisitic regression
# First recode the distress_ct variable into 0 and 1, making 1 to represent at least
# one failure during a launch.
launch$distress_ct = ifelse(launch$distress_ct<1,0,1)
launch$distress_ct
# set up trainning and test data sets
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx
launch_train = launch[indx,]
launch_test = launch[-indx,]
launch_train_labels = launch[indx,1]
launch_test_labels = launch[-indx,1]
# Check if there are any missing values
library(Amelia)
missmap(launch, main = "Missing values vs observed")
# number of missing values in each column
sapply(launch,function(x) sum(is.na(x)))
# number of unique values in each column
sapply(launch, function(x) length(unique(x)))
# fit the logistic regression model, with all predictor variables
model <- glm(distress_ct ~.,family=binomial(link='logit'),data=launch_train)
model
model <- glm(distress_ct ~.,family=binomial(link='logit'),data=launch_train)
model
summary(model)
anova(model, test="Chisq")
anova(model, test="Chisq")
model <- glm(distress_ct~temperature,family=binomial(link='logit'),data=launch_train)
model
summary(model)
anova(model, test="Chisq")
# drop the insignificant predictors, alpha = 0.10
model <- glm(distress_ct~temperature,family=binomial(link='logit'),data=launch_train)
model
summary(model)
anova(model, test="Chisq")
model1 <- glm(distress_ct~temperature,family=binomial(link='logit'),data=launch_train)
model1
anova(model,model1)
model <- glm(distress_ct ~.,family=binomial(link='logit'),data=launch_train)
model
anova(model,model1)
anova (model, model1)
model <- glm(distress_ct~temperature,family=binomial(link='logit'),data=launch_train)
model
summary(model)
anova(model, test="Chisq")
fitted.results <- predict(model,newdata=launch_test,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != launch_test$distress_ct)
print(paste('Accuracy',1-misClasificError))
# check Accuracy
fitted.results <- predict(model,newdata=launch_test,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != launch_test$distress_ct)
print(paste('Accuracy',1-misClasificError))
